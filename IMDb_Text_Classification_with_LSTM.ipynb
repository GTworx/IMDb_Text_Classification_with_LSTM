{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GTworx/IMDb_Text_Classification_with_LSTM/blob/main/IMDb_Text_Classification_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "IMDb Text Classification with LSTM.\n",
        "\n",
        "This script demonstrates how to build, train, and evaluate a Long Short-Term\n",
        "Memory (LSTM) neural network for sentiment analysis on the IMDb movie review\n",
        "dataset.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Import Necessary Libraries\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Set Hyperparameters and Load Data\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "# The number of most frequent words to consider in the dataset.\n",
        "# Words are ranked by frequency, so only the top `top_words` are kept.\n",
        "TOP_WORDS = 5000\n",
        "# The maximum number of words to use in each movie review.\n",
        "# If a review is shorter, it will be padded; if longer, it will be truncated.\n",
        "MAX_REVIEW_LENGTH = 500\n",
        "# The dimension of the word embeddings. Each word will be represented\n",
        "# by a vector of this size.\n",
        "EMBEDDING_VECTOR_LENGTH = 32\n",
        "# The number of memory units in the LSTM layer.\n",
        "LSTM_UNITS = 100\n",
        "\n",
        "# --- Load the IMDb Dataset ---\n",
        "# The dataset is pre-processed, where each review is a sequence of word\n",
        "# indexes (integers). The `num_words` argument ensures that we only load\n",
        "# words that are among the `TOP_WORDS` most frequent.\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=TOP_WORDS)\n",
        "\n",
        "print(f\"--- Data Loading ---\")\n",
        "print(f\"Number of training samples: {len(X_train)}\")\n",
        "print(f\"Number of testing samples: {len(X_test)}\")\n",
        "print(\"\\nExample of a raw training review (sequence of word indexes):\")\n",
        "print(X_train[0])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Preprocess the Data\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# --- Pad Sequences ---\n",
        "# Neural networks require inputs of a consistent shape. Since movie reviews\n",
        "# have different lengths, we need to pad or truncate them to be of\n",
        "# `MAX_REVIEW_LENGTH`.\n",
        "# `pad_sequences` is a utility function that transforms a list of sequences\n",
        "# into a 2D NumPy array of shape (num_samples, num_timesteps).\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=MAX_REVIEW_LENGTH)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=MAX_REVIEW_LENGTH)\n",
        "\n",
        "print(f\"\\n--- Data Preprocessing ---\")\n",
        "print(f\"Shape of training data after padding: {X_train.shape}\")\n",
        "print(f\"Shape of testing data after padding: {X_test.shape}\")\n",
        "print(\"\\nExample of a padded training review:\")\n",
        "print(X_train[0])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Build the LSTM Model\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# --- Define the Model Architecture ---\n",
        "# We will use a Keras Sequential model, which is a linear stack of layers.\n",
        "model = Sequential()\n",
        "\n",
        "# --- Layer 1: Embedding Layer ---\n",
        "# This layer takes the integer-encoded vocabulary and looks up the embedding\n",
        "# vector for each word index. The result is a 3D tensor of shape:\n",
        "# (batch_size, sequence_length, embedding_dim).\n",
        "# It's a crucial first step for processing text data in deep learning.\n",
        "model.add(Embedding(TOP_WORDS, EMBEDDING_VECTOR_LENGTH, input_length=MAX_REVIEW_LENGTH))\n",
        "\n",
        "# --- Layer 2: LSTM Layer ---\n",
        "# The core of our model. The LSTM layer processes the sequence of word\n",
        "# embeddings and learns to capture long-term dependencies in the text.\n",
        "# It returns a single output vector for the final timestep.\n",
        "model.add(LSTM(LSTM_UNITS))\n",
        "\n",
        "# --- Layer 3: Output Layer ---\n",
        "# A standard fully connected (Dense) layer with a 'sigmoid' activation\n",
        "# function. The sigmoid function outputs a value between 0 and 1, which\n",
        "# is perfect for binary classification (positive vs. negative sentiment).\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Compile the Model\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# --- Set Optimizer, Loss Function, and Metrics ---\n",
        "# - Optimizer: 'adam' is an efficient and commonly used optimization algorithm.\n",
        "# - Loss Function: 'binary_crossentropy' is the standard loss function for\n",
        "#   binary classification problems.\n",
        "# - Metrics: 'accuracy' will be used to monitor the model's performance.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# --- Display Model Summary ---\n",
        "# This provides a clear overview of the model's architecture, including the\n",
        "# layers, output shapes, and number of parameters.\n",
        "print(\"\\n--- Model Architecture ---\")\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6. Train the Model\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Training the Model ---\")\n",
        "# The `fit` method trains the model for a fixed number of epochs (iterations\n",
        "# over the entire dataset).\n",
        "# - epochs: The number of times to iterate over the entire training dataset.\n",
        "# - batch_size: The number of samples per gradient update.\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 7. Evaluate the Model\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# --- Calculate Final Accuracy on Test Data ---\n",
        "# The `evaluate` method returns the loss value and metric values for the model\n",
        "# in test mode.\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Accuracy on test data: {scores[1]*100:.2f}%\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 8. Make Predictions on New Data\n",
        "# -----------------------------------------------------------------------------\n",
        "# To show how to use the model in a real-world scenario, let's classify a\n",
        "# few example reviews.\n",
        "\n",
        "# --- Example Reviews (as sequences of word indexes) ---\n",
        "# In a real application, you would need a tokenizer to convert raw text\n",
        "# into these sequences based on the original IMDb word index.\n",
        "# For this example, we'll just use a few samples from the test set.\n",
        "sample_positive_review = X_test[1:2] # A known positive review\n",
        "sample_negative_review = X_test[3:4] # A known negative review\n",
        "\n",
        "# --- Make Predictions ---\n",
        "prediction_positive = model.predict(sample_positive_review)\n",
        "prediction_negative = model.predict(sample_negative_review)\n",
        "\n",
        "print(\"\\n--- Making Predictions ---\")\n",
        "print(f\"Prediction for a positive review (raw output): {prediction_positive[0][0]:.4f}\")\n",
        "print(f\"Sentiment: {'Positive' if prediction_positive[0][0] > 0.5 else 'Negative'}\")\n",
        "\n",
        "print(f\"\\nPrediction for a negative review (raw output): {prediction_negative[0][0]:.4f}\")\n",
        "print(f\"Sentiment: {'Positive' if prediction_negative[0][0] > 0.5 else 'Negative'}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "--- Data Loading ---\n",
            "Number of training samples: 25000\n",
            "Number of testing samples: 25000\n",
            "\n",
            "Example of a raw training review (sequence of word indexes):\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "\n",
            "--- Data Preprocessing ---\n",
            "Shape of training data after padding: (25000, 500)\n",
            "Shape of testing data after padding: (25000, 500)\n",
            "\n",
            "Example of a padded training review:\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
            "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
            "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
            "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
            "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
            "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16    2   19  178   32]\n",
            "\n",
            "--- Model Architecture ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training the Model ---\n",
            "Epoch 1/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 1s/step - accuracy: 0.7085 - loss: 0.5407 - val_accuracy: 0.8569 - val_loss: 0.3577\n",
            "Epoch 2/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 950ms/step - accuracy: 0.8549 - loss: 0.3509 - val_accuracy: 0.8648 - val_loss: 0.3201\n",
            "Epoch 3/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 919ms/step - accuracy: 0.8856 - loss: 0.2839 - val_accuracy: 0.8700 - val_loss: 0.3384\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Accuracy on test data: 87.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\n",
            "--- Making Predictions ---\n",
            "Prediction for a positive review (raw output): 0.9814\n",
            "Sentiment: Positive\n",
            "\n",
            "Prediction for a negative review (raw output): 0.9445\n",
            "Sentiment: Positive\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8brOW_uwwEMh",
        "outputId": "d84409c7-41c3-4b3d-f443-463547281d87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 2.5. Decode a Review to See Text Content\n",
        "# -----------------------------------------------------------------------------\n",
        "# To see the actual words, we need to retrieve the word-to-index mapping\n",
        "# provided by Keras.\n",
        "word_to_index = imdb.get_word_index()\n",
        "\n",
        "# The indexes are offset by 3 because 0, 1, and 2 are reserved for special\n",
        "# tokens: '<PAD>', '<START>', and '<UNK>' (unknown).\n",
        "index_to_word = {value + 3: key for key, value in word_to_index.items()}\n",
        "index_to_word[0] = \"<PAD>\"\n",
        "index_to_word[1] = \"<START>\"\n",
        "index_to_word[2] = \"<UNK>\"\n",
        "\n",
        "def decode_review(text_sequence):\n",
        "    \"\"\"Converts a sequence of word indexes back into a readable string.\"\"\"\n",
        "    return \" \".join([index_to_word.get(i, \"?\") for i in text_sequence])\n",
        "\n",
        "# Let's decode the first training review to see the text.\n",
        "print(\"\\nExample of a decoded training review:\")\n",
        "print(decode_review(X_train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMfShjni11yk",
        "outputId": "12d7ad87-d1e4-4987-f7a7-cfe1c797dd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Example of a decoded training review:\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}